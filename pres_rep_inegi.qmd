---
# title: "Mesa de Procesamiento REP"
# author: "Febrero 2024"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---
#

<!---
# TODO: this does not work
.linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
<img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**REP y la explotación de RRAA para su uso estadístico a partir de un lago de datos**]{.big-par .center-justified}

[**Abril 2024**]{.big-par .center-justified}

<!---
## PE Servicios Compartidos  
--->

## Temas: 

::: {.incremental .medium-par}

- Objetivos del Registro Estadístico de Población (REP)
- Estrategia y procesamiento de las bases
- Próximos pasos
- Demo
- Preguntas y/o comentarios

:::

## Objetivos del REP

::: {.incremental .medium-par}
- El Registro Estadístico de Población (REP) busca compilar datos de la población permitiendo su **conteo, localización y caracterización demográfica básica**.
- Se construye a partir de datos provenientes de registros administrativos (RRAA), tomando en cuenta el procesamiento necesario de dichas fuentes de información para su uso estadístico.
- Busca complementar la información proveniente de censos y encuestas.
- En el largo plazo, el objetivo es reemplazar al censo
:::

::: notes
- En resumen, el REP aydudará a contar y caracterizar a la población, disminuyendo los costos asociados al levantamiento de datos y también reducir las cargas de los entrevistadores e informantes, entre otros beneficios.
:::

## Estrategia y procesamiento de las bases

::: {.incremental .medium-par}
- Contamos con tres RRAA como fuentes de información:
  - **Servicio de Registro Civil e Identificación (SRCeI)**: contiene todos los registros de personas que han sacado un documento de identificación.
  - **Fondo Nacional de Salud (FONASA)**: contiene información sobre los beneficiarios de la salud pública.
  - **Superintendencia de Seguridad Social (SUSESO)**: contiene el detalle de los antecedentes generales de todos los trabajadores cubiertos por la Superintendencia 
- El acceso a RRAA nominados nos permite hacer la vinculación determinística de estas bases utilizando el RUN de las personas.
:::

::: notes
- El Registro Civil se considera como nuestra base pivote ya que posee un compilado de todos los RUNes otorgados en el país, ya sean chilenos o extranjeros. Esta base contiene más de 22 millones de filas.
- El Fondo Nacional de Salud contiene información sobre los beneficiarios de la salud pública y es la segunda base con mayor información ya que son más de 15 millones de registros por mes.
- La Superintendencia de Seguridad Social es la tercera fuente de información con cerca de 9 millones de registros.
:::

## Estrategia y procesamiento de las bases
#### Estrategia
::: {.incremental .medium-par}
- **Mayor desafío**: volúmen de los datos
- **Estrategia**: Apache Arrow
  - Es una plataforma de desarrollo para análisis en memoria.
  - Proporciona un formato de memoria columnar estandarizado para compartir datos de manera eficiente y realizar análisis rápidos.
  - Utiliza un enfoque agnóstico al lenguaje, lo que elimina la necesidad de convertir los datos a un formato específico para diferentes lenguajes de programación, mejorando así el rendimiento y la interoperabilidad entre sistemas y procesos de datos complejos.
- **Limitaciones**: 
  - Escasa variedad de funciones disponibles para usar en R.
  - Computacionalmente sigue siendo muy costoso.
:::

::: notes
- Muchas veces tenemos que ponernos creativos para realizar las operaciones que queremos cuando no encontramos funciones que sean compatibles con Arrow, lo que hace que tengamos que agregar pasos adicionales para procesar las bases.
- También, nos hemos dado cuenta que computacionalmente sigue siendo bastante costoso correr todo el flujo de procesamiento por lo que estamos buscando alternativas a esta estrategia.
:::

## Estrategia y procesamiento de las bases
::: {.incremental .medium-par}
- Trabajar con fechas hace que el procesamiento sea más lento
:::
. . .

```{r}
#| eval: FALSE
#| echo: TRUE
data <- data %>% mutate(fecha_nac_corregida = paste0(anio_nacimiento,"-",mes_nacimiento2,"-",dia_nacimiento2),
                          fecha_nac_corregida = ymd(fecha_nac_corregida),
                          fecha_def_corregida = paste0(anio_defuncion,"-",mes_defuncion,"-",dia_defuncion),
                          fecha_def_corregida = ymd(fecha_def_corregida)) %>% 
    compute()

```

::: {.incremental .medium-par}
- Verse obligados a trear los datos a R
:::

. . .

```{r}
#| eval: FALSE
#| echo: TRUE

iqr <- arrow_table %>%
    summarise(iqr = IQR(edad, 0.75, na.rm = T)) %>%
    collect()
    
```

::: notes
- Acá tenemos algunos ejemplos de códigos que demoran en correr
- En el primer ejemplo estamos re construyendo la variable fecha de nacimiento, luego de haber corregido el día, el mes y el año de nacimiento. Esta variable la tenemos que convertir en formato fecha. En general, todas las operaciones que involucren fechas demoran más de lo normal.
- En el segundo ejemplo tenemos que traer los datos a R porque la función IQR que calcula el rango intercuantilico no está disponible en Arrow. Esto también genera que el procesamiento sea más lento
:::

## Estrategia y procesamiento de las bases
#### Procesamiento
::: {.incremental .medium-par}
- **Primera etapa**: descarga
- **Segunda etapa**: exploración y cálculo de indicadores de calidad.
  - Tasa de completitud
  - Tasa de valores atípicos
  - Proporción de registros duplicados
  - Validaciones del RUT
- **Tercera etapa**: limpieza y procesamiento de las bases.
  - Corrección de fechas.
  - Homologación de variables.
  - Normalización de nombres.
  - Remover valores atípicos y runes inválidos.
  - Deduplicación
:::

::: notes
- Descarga: actualmente las bases están en un formato de texto plano en un server de SQL y hacemos las consultas para descargarlas en formato feather y trabajar con ellas mediante R usando Arrow y Targets
- La segunda etapa corresponde a la exploración y cálculo de indicadores de calidad. Con respecto a los indicadores de calidad tenemos:
  - La tasa de completitud que corresponde a la proporción de celdas con datos sobre el total de celdas y nos indica qué tan completa es la información recopilada.
  - La tasa de valores atípicos donde nos enfocamos principalmente en la variable "edad". Acá establecimos un umbral para detectar estos casos y en el RC se encontraron cerca de un 4% de regustros por sobre este umbral.
  - Proporción de registros duplicados. En el caso de FONASA y el RC se encontraron tasas muy cercanas al 0% lo cual es un buen indicador.
  - Validaciones del RUT. Acá comprobamos que el número de caracteres de los RUNES estén dentro del largo aceptable y que el dígito verificador sea el que corresponde
- La tercera etapa consitió en la limpieza y el procesamiento de las bases:
  - Corrección de fechas
  - Homologación de variables
  - Normalización de nombres
  - Remover valores atípicos [hacemos el supuesto de que las personas por sobre el umbral de la variable edad son personas fallecidas] y runes inválidos.
  - Deduplicación [por el momento se hace eligiendo un registro de manera aleatoria. más adelantes haremos pruebas usando el paquete SPLINK que mencionaré más adelante]
:::

## Estrategia y procesamiento de las bases
#### Procesamiento
::: {.incremental .medium-par}
- **Cuarta etapa**: vinculación determinística mediante el RUT.
  - 96,3% de los registros de SUSESO hicieron match.
  - 95,1% de los registros en FONASA hicieron match.
  - 70% de los registros que hicieron match comparten el nombre con el Registro Civil.
- **Quinta etapa**: integración de los registros que no hicieron match en el paso anterior.
- **Sexta etapa**: contabilización y caracterización final de personas.
:::

::: notes
- En la cuarta etapa se realiza la vinculación determinística usando el RUT. Para esto se uso el Registro Civil como base pivote y se realizó el merge con FONASA y SUSESO. El resultado fue un match parcial, es decir, existen registros dentro de SUSESO y FONASA para los cuales no se encontró un pareo en la tabla pivote del Registro Civil
- Hasta el momento el RUT ha demostrado ser una buena variable para vincular personas. Cerca del 70% de los registros que hacen match comparten el mismo nombre con las otras bases. Esto es un aproximación a nuestros verdaderos postivos, lo cual es un buen punto de partida.
- En la quinta etapa, se integraron todos los registros que no hicieron match en el paso anterior, es decir, se sumaron a la base final.
- Finalmente, la sexta etapa corresponde a la contabilización y caracterización final de personas, donde quedamos con una tabla con más de 22 millones de registros y diferentes variables sociodemográficas tales como el sexo, la edad, la nacionalidad, entre otras.
:::

## Próximos pasos
::: {.incremental .medium-par}
- Integrar más registros administrativos.
- Perfeccionar la identificación de a quienes sumar y a quienes restar (por ejemplo identificación de residentes usando señales de vida). 
- Desarrollo y pruebas de vinculación probabilística utilizando el paquete **SPLINK**.
  - Paquete de Python para vinculación probabilística de registros.
  - Permite deduplicar y vincular millones de registros.
  - Cacula la probabilidad de que dos registros aleatorios sean iguales.
:::

. . . 

::: {.incremental .medium-par layout-ncol=2 layout-valign="center"}

![](imagenes/Captura.PNG)

![](imagenes/Captura2.PNG)

:::

::: notes
- Integrar más RRAA al procesamiento
- Perfeccionar la identificación de a quienes sumar y a quienes restar (por ejemplo identificación de residentes usando señales de vida). [Consideramos como señal de vida la aparición continua de una persona en las diferentes bases]
- Desarrollo y pruebas de vinculación probabilística utilizando el paquete SPLINK:
  - Splink es un paquete de Python para vinculación probabilística que permite deduplicar y vincular millones de registros incluso sin un id único de persona.
  - Lo que hace es comparar pares de registros y decidir cuáles son matches y cuáles no mediante reglas de comparación. Por ejemplo, queremos comparar todos los     registros que tengan la misma fecha de nacimiento y las tres primeras letras primer nombre. Luego de hacer la comparación se calcula un score de coincidencia que
  es un predicción para determinar si dos registros son o no la misma persona, aplicando pesos a las variables. Por ejemplo, el sexo puede entregar menos información que la fecha de nacimiento.
  - Para nosotros esta es una solución para deduplicar a personas que pueden ser las mismas pero tienen un RUT distinto por diversas razones. Por ejemplo, en FONASA a las personas que se atienden en un establecimiento de salud y que no están registradas en el Registro Civil se les da un RUT provisorio pero luego esas personas podrían registrarse en el RC y quedarán registrados con otro RUT, generando que estas personas queden duplicadas en nuestra base.
:::

## Próximos pasos
#### Lago de datos
::: {.incremental .medium-par}
- **Objetivo a corto plazo**: pasar la base de datos final a MinIO y hacer consultas mediante Trino desde la tabla virtualizada
:::

![](imagenes/flujo.PNG)

::: notes
- La idea es traspasar todas las bases brutas a Minio, luego mediante Trino traer las bases y hacer el procesamiento mediante R o Python. El próximo paso es traspasar esa base final nuevamente al lago de datos usando Trino y finalmente hacer diversos análisis usando Superset u alguna otra herramienta de análisis.
:::


## Demo usando Minio/Trino

::: {.incremental .medium-par}


:::

## Preguntas y/o comentarios
![](imagenes/flujo.PNG)

#

<!---
# TODO: this does not work
.linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**REP y la explotación de RRAA para su uso estadístico a partir de un lago de datos**]{.big-par .center-justified}

[**Abril 2024**]{.big-par .center-justified}


[]{.linea-superior} 
[]{.linea-inferior} 